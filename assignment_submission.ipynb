{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c67ca928",
      "metadata": {},
      "source": [
        "# Cell Instance Segmentation with UNI-style Backbone (BCCD)\n",
        "\n",
        "This notebook provides a reproducible pipeline for the assignment:\n",
        "- data split + preprocessing\n",
        "- model training\n",
        "- instance segmentation post-processing\n",
        "- evaluation (IoU, Dice, mAP)\n",
        "- qualitative success/failure analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19624fa2",
      "metadata": {},
      "source": [
        "## 1) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e2dc612",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import os\n",
        "\n",
        "ROOT = Path('.').resolve()\n",
        "CKPT = ROOT / 'github_uni/checkpoints_single/best_model.pt'\n",
        "EVAL_DIR = ROOT / 'github_uni/checkpoints_single/instance_eval_test'\n",
        "print('ROOT:', ROOT)\n",
        "print('Checkpoint exists:', CKPT.exists())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2445fba",
      "metadata": {},
      "source": "## 2) Data Preparation (Train/Val/Test + EDT)\nUse provided `test/` as hold-out. Split only `train/` into train/val for model selection.\n\nImage preprocessing and augmentation strategy:\n- Resize to `224x224` (ViT patch-compatible resolution).\n- Normalize with ImageNet mean/std to match pretrained encoder statistics.\n- Generate EDT supervision from binary masks (`generate_edt.py`).\n- Training-time augmentation (conservative):\n  - horizontal/vertical flip\n  - 90/180/270 rotation\n  - no aggressive color jitter/elastic transforms by default\n\nWhy conservative augmentation (pathology cell segmentation):\n- Cell morphology and boundary texture are key signals; aggressive distortions can corrupt those cues.\n- Strong synthetic color changes may create unrealistic stain distributions.\n- Rotation/flip are label-preserving and biologically plausible for patch-level cell images.\n- With a strong pretrained backbone and limited data, conservative augmentation is usually more stable.\n\nWhy no explicit stain normalization in this baseline:\n- First objective was a stable and reproducible baseline with minimal extra preprocessing.\n- BCCD color variation is moderate relative to multi-center pathology cohorts.\n- Stain normalization can introduce texture shifts/artifacts if not tuned carefully.\n- We leave Reinhard/Macenko normalization as a controlled follow-up ablation.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9487309b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run once (terminal or notebook shell):\n",
        "# !python3 github_uni/datasets/create_split.py \\\n",
        "#   --train_original \"data/BCCD Dataset with mask/train/original\" \\\n",
        "#   --train_mask \"data/BCCD Dataset with mask/train/mask\" \\\n",
        "#   --val_ratio 0.2 --seed 8888 \\\n",
        "#   --out_json \"github_uni/datasets/splits/bccd_train_val_split.json\"\n",
        "\n",
        "# !python3 github_uni/datasets/generate_edt.py \\\n",
        "#   --dataset_root \"data/BCCD Dataset with mask\" --d_max 15"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed7eb6df",
      "metadata": {},
      "source": "## 3) Model Adaptation and Design (Pathology Context)\n- **Backbone**:\n  - Current reported results use `vit_base_patch16_224`.\n  - Pipeline is architecturally aligned with UNI-style adaptation (tokenized encoder -> dense decoder), but pretrained weights differ.\n- **Token-to-map adaptation**: remove prefix/register tokens, reshape patch tokens into 2D feature map.\n- **Decoder (lightweight by design)**:\n  - `1x1` channel projection from encoder embedding to decoder width\n  - 3-stage `bilinear interpolate + Conv-BN-ReLU`\n  - final resize to full image resolution\n  - decoder normalization: **BatchNorm**\n- **Why interpolate + Conv-BN-ReLU (not deconv)**:\n  - avoids checkerboard artifacts often seen with deconvolution\n  - produces smoother boundaries for thin/overlapping cell edges\n  - lower complexity and faster iteration with frozen encoder\n- **Dual heads (shared decoder feature input)**:\n  - semantic head (`seg_logits`): pixel-wise cell/background classification\n  - distance head (`dist_pred`): EDT regression for touching-cell separation cues\n- **Instance step**: threshold foreground + smooth distance + local peaks + watershed.\n\nDesign rationale:\n- Small crowded cells need strong global features + stable local refinement.\n- A simple decoder avoids overfitting on BCCD while preserving boundary detail.\n- EDT head targets the overlap problem where semantic masks alone tend to merge cells.\n"
    },
    {
      "cell_type": "markdown",
      "id": "f4e55ada",
      "metadata": {},
      "source": "## 4) Training and Hyperparameter Strategy\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52b7bce2",
      "metadata": {},
      "outputs": [],
      "source": "# Train command (example):\n# !python3 github_uni/main_single.py\n\n# Baseline hyperparameters\nBASELINE_CFG = {\n    'seed': 8888,\n    'lr': 2e-4,\n    'wd': 1e-4,\n    'batch_size': 4,\n    'img_size': 224,\n    'max_num_epochs': 50,\n    'dist_weight': 0.5,\n}\n\n# Tuning notes\n# Exp-2: lr=1e-4, wd=5e-5 (kept other settings fixed)\n# Result: slight degradation vs baseline on this split.\n# Exp-3 (running): lr=1e-4, wd=1e-4, dist_weight=0.3\n# Goal: reduce over-segmentation and improve instance precision/AJI.\n\n# Important tuning principle:\n# balance two-head performance jointly:\n# - higher dist_weight can improve splitting but may increase false positives / over-splitting\n# - lower dist_weight can protect semantic Dice/IoU but weaken touching-cell separation\n# therefore monitor Dice/IoU + AP/AJI/F1@0.5 together.\n\nBASELINE_CFG\n"
    },
    {
      "cell_type": "markdown",
      "id": "b7d55136",
      "metadata": {},
      "source": "## 5) Baseline + Ablation Checkpoint Metrics\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78da23d6",
      "metadata": {},
      "outputs": [],
      "source": "baseline_metrics = {\n    'best_val': {'loss': 0.29771364391860317, 'f1': 0.9237635902519266, 'f1_0': 0.9530601222049384, 'f1_1': 0.8944670582989146},\n    'best_test': {'loss': 0.3000761054456234, 'f1': 0.9239783591861196, 'f1_0': 0.9521112985077236, 'f1_1': 0.8958454198645156}\n}\n\nexp2_metrics = {\n    'best_val': {'loss': 0.30855946848958227, 'f1': 0.9207980251792034, 'f1_0': 0.9510220371592142, 'f1_1': 0.8905740131991925},\n    'best_test': {'loss': 0.3117054454982281, 'f1': 0.920871966296492, 'f1_0': 0.9499723239967988, 'f1_1': 0.8917716085961853}\n}\n\nexp3_metrics = {\n    'best_val': {'loss': 0.2980442628011865, 'f1': 0.9228892796054556, 'f1_0': 0.952499013768235, 'f1_1': 0.8932795454426763},\n    'best_test': {'loss': 0.3008011419326067, 'f1': 0.9230502689125806, 'f1_0': 0.951569863205416, 'f1_1': 0.8945306746197451}\n}\n\nbaseline_metrics, exp2_metrics, exp3_metrics\n"
    },
    {
      "cell_type": "markdown",
      "id": "2f9eaff5",
      "metadata": {},
      "source": [
        "## 6) Instance Segmentation + Evaluation (IoU, Dice, mAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce8bb5da",
      "metadata": {},
      "outputs": [],
      "source": "# Baseline instance evaluation\n# !python3 github_uni/eval/evaluate_instance.py #   --checkpoint github_uni/checkpoints_single/best_model.pt #   --split test #   --save_dir github_uni/checkpoints_single/instance_eval_test\n\n# Exp-2 instance evaluation\n# !python3 github_uni/eval/evaluate_instance.py #   --checkpoint github_uni/checkpoints_single2/best_model.pt #   --split test #   --save_dir github_uni/checkpoints_single2/instance_eval_test\n\n# Exp-3 instance evaluation\n# !python3 github_uni/eval/evaluate_instance.py #   --checkpoint github_uni/checkpoints_single3/best_model.pt #   --split test #   --save_dir github_uni/checkpoints_single3/instance_eval_test\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d01267ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_path = EVAL_DIR / 'metrics_summary.json'\n",
        "if summary_path.exists():\n",
        "    summary = json.loads(summary_path.read_text())\n",
        "    print(json.dumps(summary, indent=2)[:2000])\n",
        "else:\n",
        "    print('Run evaluation first, then re-run this cell.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1ef8518",
      "metadata": {},
      "source": "## 7) Metrics Table (IoU, Dice, mAP)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89cde071",
      "metadata": {},
      "outputs": [],
      "source": "import json\nfrom pathlib import Path\n\ndef load_summary(p):\n    p = Path(p)\n    return json.loads(p.read_text()) if p.exists() else None\n\ns1 = load_summary('github_uni/checkpoints_single/instance_eval_test/metrics_summary.json')\ns2 = load_summary('github_uni/checkpoints_single2/instance_eval_test/metrics_summary.json')\ns3 = load_summary('github_uni/checkpoints_single3/instance_eval_test/metrics_summary.json')\n\nrows = []\nfor name, s in [('Baseline', s1), ('Exp-2', s2), ('Exp-3', s3)]:\n    if s is None:\n        continue\n    rows.append({\n        'Run': name,\n        'Dice': round(s['semantic']['dice_fg'], 4),\n        'IoU': round(s['semantic']['iou_fg'], 4),\n        'AP50': round(s['detection']['AP@0.50'], 4),\n        'mAP(0.50:0.95)': round(s['detection']['mAP_50_95'], 4),\n        'Inst_F1@0.5': round(s['instance']['f1'], 4),\n        'AJI': round(s['instance']['aji'], 4),\n    })\n\nrows\n"
    },
    {
      "cell_type": "markdown",
      "id": "4a4b60d5",
      "metadata": {},
      "source": [
        "## 8) Qualitative Visualizations (3-5 cases)\n",
        "The evaluation script exports overlays to:\n",
        "- `.../qualitative/success`\n",
        "- `.../qualitative/failure`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7159a66",
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "\n",
        "def show_first_n(folder, n=3):\n",
        "    folder = Path(folder)\n",
        "    files = sorted(folder.glob('*.png'))[:n]\n",
        "    print(folder, 'count=', len(files))\n",
        "    for f in files:\n",
        "        print(f.name)\n",
        "        display(Image.open(f))\n",
        "\n",
        "show_first_n(EVAL_DIR / 'qualitative' / 'success', n=3)\n",
        "show_first_n(EVAL_DIR / 'qualitative' / 'failure', n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae7eecca",
      "metadata": {},
      "source": "## 9) Interpretation of Success/Failure Cases and Main Challenge\nFor each selected case, briefly comment on:\n- foreground overlap quality (Dice/IoU behavior)\n- split quality for touching cells (mAP/AJI behavior)\n- likely error source: clumping, weak boundaries, tiny objects, staining/background variation\n\nMain challenge: **cell overlap / touching cells**.\n- In our runs, semantic quality is strong, but instance metrics are moderate.\n- This indicates foreground is detected well, yet separation of adjacent cells is still imperfect.\n- Precision lower than recall suggests occasional over-segmentation in crowded regions.\n\nTrade-off reminder:\n- higher Dice/IoU does not always imply higher instance mAP\n- instance metrics are more sensitive to merge/split errors than semantic metrics.\n"
    },
    {
      "cell_type": "markdown",
      "id": "539a3492",
      "metadata": {},
      "source": "## 10) Reflection and Next Improvements\n- Replace fallback backbone with UNI2-h when gated access is approved.\n- Add explicit stain normalization (e.g., Reinhard/Macenko) and compare against current normalization-only pipeline.\n- Decoder ablation: BatchNorm vs GroupNorm (not tested due to time).\n- Explore stronger semantic losses (e.g., Focal loss) to improve class-decision balance in practice.\n- Test more aggressive augmentation in controlled studies (e.g., random resized crop, mild color jitter, blur).\n- Tune watershed post-processing (`min_distance`, `peak_threshold`, `area_min`).\n- Add stronger instance-aware supervision for overlap regions:\n  - example: center-heatmap auxiliary head or boundary-aware auxiliary head.\n- Use Ray Tune for broader hyperparameter search under frozen-encoder lightweight training.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}